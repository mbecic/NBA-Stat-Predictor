{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Pts Ast Reb 2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out players who played less than 10 career games\n",
    "data = data.groupby('Player_(POI_Game_Stats)').filter(lambda x: len(x) > 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Points and Assists Since they are not needed to predict points\n",
    "data = data.drop(['PTS','AST'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only year age of players and dropping number of days\n",
    "def year_age(row,column):\n",
    "    year = int(row[column][0:2])\n",
    "    return year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age_(POI_Game_Stats)'] = data.apply(year_age,args=('Age_(POI_Game_Stats)',), axis=1)\n",
    "data['Age_(DOI_Game_Stats)'] = data.apply(year_age,args=('Age_(DOI_Game_Stats)',), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Date columne to Datetime type\n",
    "data['Date'] = pd.to_datetime(data['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting in descending order of dates, so when i get the rolling mean in the next code block, it's for the past X games\n",
    "\n",
    "data = data.sort_values('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Trend for average number of points in the past i games\n",
    "def past_X_games(i,df):\n",
    "    group = df.groupby('Player_(POI_Game_Stats)')['TRB'].apply(lambda x: x.shift().rolling(i).mean()).reset_index()\n",
    "    column_name = \"REB_{}\".format(i)\n",
    "    thing = group.set_index('index').rename(columns={\"TRB\":column_name})\n",
    "    return thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_days = [3,5,7,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Average points in past 3,5,7 and 10 games\n",
    "for i in moving_days:\n",
    "    window_av = past_X_games(i,data)\n",
    "    data = data.join(window_av,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only month of date\n",
    "\n",
    "data['Date'] = data['Date'].dt.strftime('%b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only the year the season began\n",
    "\n",
    "def season(row):\n",
    "    return int(row['Season'][0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Season'] = data.apply(season,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating binary variables for Position and Home/Away since they both ony have 2 options\n",
    "\n",
    "data['Pos'] = le.fit_transform(data['Pos'])\n",
    "data['Home/Away'] = le.fit_transform(data['Home/Away'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Player_(POI_Game_Stats)','Tm_(POI_Game_Stats)','Opp','Player_(DOI_Game_Stats)', 'Season'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy variables for categorical data, ie the month of the game played\n",
    "\n",
    "dummy = pd.get_dummies(data['Date'])\n",
    "data = data.drop('Date',axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.join(dummy, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out features that have limited correlation to the Pts scored in a game\n",
    "\n",
    "for i in data.columns.values:\n",
    "    cor = abs(data['TRB'].corr(data[i]))\n",
    "    #cor = str(cor)\n",
    "    if cor > 0.1:\n",
    "        continue\n",
    "    else:\n",
    "        del data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['TRB'],axis=1)\n",
    "y = data['TRB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out features that are largely constant or change minimaly\n",
    "\n",
    "sel = VarianceThreshold()\n",
    "vt = sel.fit(X)\n",
    "X = X.iloc[:, vt.variances_ > 0.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,RandomizedSearchCV, KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Simple Linear Regression Model and Gradient Boosting Regression Model\n",
    "\n",
    "gbr = xgb.XGBRegressor(max_depth=5,n_estimators=250,learning_rate=0.01)\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test ,y_train,y_test = train_test_split(X,y,test_size=0.3,train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr.fit(X_train, y_train)\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test,gbr.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test,lr.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe of the features importances from the gradient boosting model\n",
    "\n",
    "feature_importances = pd.DataFrame([gbr.feature_importances_], columns=X.columns.values).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keeping features that have importance more than 0.01\n",
    "\n",
    "important_features = feature_importances[feature_importances[0]>0.01].reset_index()['index'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all features that were not important features\n",
    "\n",
    "X_ = X[X.columns.intersection(important_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test, Train split with only the important features\n",
    "\n",
    "X_train,X_test ,y_train,y_test = train_test_split(X_,y,test_size=0.3,train_size=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr.fit(X_train, y_train)\n",
    "mean_squared_error(y_test,gbr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning the number of estimators and the learning rate\n",
    "\n",
    "parameters = {\"n_estimators\": [100,250,500,750], \"learning_rate\": [0.05,0.1,0.2]}\n",
    "\n",
    "grid = GridSearchCV(gbr,parameters, scoring='neg_mean_squared_error',cv=3)\n",
    "\n",
    "grid.fit(X_,y)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "# We see ideal learning rate is 0.05 and number of estimators is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning the max depth and the min child weight of the model\n",
    "\n",
    "parameters = {\"max_depth\":[3,5,7,9], \"min_child_weight\":[1,3,5]}\n",
    "\n",
    "grid = GridSearchCV(estimator=xgb.XGBRegressor(n_estimators=100,learning_rate=0.05),param_grid=parameters, scoring='neg_mean_squared_error',cv=3)\n",
    "\n",
    "grid.fit(X_,y)                    \n",
    "\n",
    "print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "# We see ideal max depth is 3 and child weight is 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = xgb.XGBRegressor(n_estimators=100,learning_rate=0.05,max_depth=3,min_child_weight=5,importance_type='gain',booster='gbtree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(gbr)\n",
    "shap_values = explainer.shap_values(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shap values of first prediction\n",
    "\n",
    "shap.force_plot(explainer.expected_value, shap_values[0,:], X_train.iloc[0,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at summary of shap values for all important features\n",
    "shap.summary_plot(shap_values, X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with 1 layer, with nodes equal to number of features\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(len(X.columns.values), input_dim=len(X.columns.values), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Model with 1 hidden layer\n",
    "def Double_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(len(X.columns.values), input_dim=len(X.columns.values), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(0.5*len(X.columns.values), kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Model with 2 hidden layers\n",
    "def Triple_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(len(X.columns.values), input_dim=len(X.columns.values), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(2*len(X.columns.values), kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dense(0.5*len(X.columns.values), kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# 1 hidden layer thats twice the size as the number of features\n",
    "def FunStuff_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(len(X.columns.values), input_dim=len(X.columns.values), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(2*len(X.columns.values), kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the First Neural Net\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=10, batch_size=500, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=3)\n",
    "results = cross_val_score(pipeline, X, y, cv=kfold, scoring=\"neg_mean_squared_error\")\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the 2nd Neural Net\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=Double_model, epochs=10, batch_size=500, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=3)\n",
    "results = cross_val_score(pipeline, X, y, cv=kfold, scoring=\"neg_mean_squared_error\")\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the 3rd Neural Net\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=Triple_model, epochs=10, batch_size=500, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=3)\n",
    "results = cross_val_score(pipeline, X, y, cv=kfold, scoring=\"neg_mean_squared_error\")\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the 4th Neural Net\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=FunStuff_model, epochs=10, batch_size=500, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=3)\n",
    "results = cross_val_score(pipeline, X, y, cv=kfold, scoring=\"neg_mean_squared_error\")\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The Gradient Boosting Regeression Model with tuned hyperparameters \n",
    "    performed the best, so thats the one that will be used in the website'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
